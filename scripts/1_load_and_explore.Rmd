---
title: "Loading epigraphic data and basic descriptive statistics"
author: 
- Petra Hermankova^[Aarhus University, Denmark, https://orcid.org/0000-0002-6349-0540]
date: "`r format(Sys.Date())`"
output:
  html_document:
    theme: united
    toc: yes
    toc_float: true
    number_sections: true
    toc_depth: 3
    df_print: paged
---

_Purpose of this script is a basic exploration of the contents of inscriptions and its attributes._

# Initial setup

```{r setup, echo=TRUE, message=FALSE, warning = FALSE}

knitr::opts_chunk$set(message = FALSE, warning = FALSE)

# Install required packages if not already installed
if (!requireNamespace("jsonlite", quietly = TRUE)) install.packages("jsonlite")
if (!requireNamespace("dplyr", quietly = TRUE)) install.packages("dplyr")
if (!requireNamespace("ggplot2", quietly = TRUE)) install.packages("ggplot2")
if (!requireNamespace("tidytext", quietly = TRUE)) install.packages("tidytext")
if (!requireNamespace("raster", quietly = TRUE)) install.packages("raster")
if (!requireNamespace("tidyverse", quietly = TRUE)) install.packages("tidyverse")
if (!requireNamespace("sf", quietly = TRUE)) install.packages("sf")
if (!requireNamespace("xml2", quietly = TRUE)) install.packages("xml2")
if (!requireNamespace("leaflet", quietly = TRUE)) install.packages("leaflet")
if (!requireNamespace("htmlwidgets", quietly = TRUE)) install.packages("htmlwidgets")

# Load necessary libraries
library(tidyverse)
library(jsonlite)
library(tidytext)
library(xml2)
library(dplyr)
library(leaflet)
library(raster)
library(sf)
library(ggplot2)
library(htmlwidgets)

```

# Working with inscriptions in R

## Display the current working directory
```{r}
getwd()
```

## List files in the current directory

```{r}
list.files()
```

## Create a variable
```{r}
x <- 10
y <- 5
z <- "dis manibus"
w <- "Pictos / victos / hostis / deleta / ludite / securi"
```

## Simple computation
```{r}
number <- x + y
number
```

## Running Existing Code and Commenting on Code

```{r}
# This is a comment. I can write whatever I want and the computer will not see it, unless I remove the ####.

# Running existing code
numbers <- c(1,3,5,6, 89, 3878) # creates a new numeric vector
summary(numbers) # provides basic stat overview of the vector
```

## Simple plot

```{r}
# Create a dataframe with variable1 and variable2
data <- data.frame(
  variable1 = c(1, 2, 5, 7, 8, 1, 33, 24, 234),
  variable2 = c(12, 2, 3, 7, 1, 1, 331, 24, 24)
)

# Display the dataframe
print(data)

# Create a plot
plot <- ggplot(data, aes(x = variable1, y = variable2)) + geom_point()
# Display the plot
plot

```

## Saving plots and resuts as CSV

```{r}
# Save the plot
ggsave("../figures/plot_test.png", plot)

# Save results to a CSV file
write.csv(data, "../data/results_test.csv")
```


# Loading epigraphic data in R

**Before You Begin**

- Make sure you have R and RStudio installed
- Install required libraries
- Ensure your data files are in the correct directory and you know how to navigate to it

## Load data from a CSV file

- Use the read.csv() function to load CSV (Comma-Separated Values) files
- Basic syntax: dataName <- read.csv("path/to/your/file.csv")
- Replace dataName with a meaningful variable name
- Replace the file path with the location of your specific CSV file

```{r}
dataCSV <- read.csv("../data/results_test.csv")
```

## Load data from a TSV file

- Use the read_tsv() function from the readr library to load TSV (Tab-Separated Values) files
- First, make sure to load the readr library with library(readr)
- Basic syntax: dataName <- read_tsv("path/to/your/file.tsv")
- Replace dataName with a meaningful variable name
- Replace the file path with the location of your specific TSV file

```{r}
tumulusTSV <- read_tsv("../data/2025-03-10-EDCS_via_Lat_Epig-term1_tumulus-74.tsv")
```

## Load data from a JSON file

- Use the fromJSON() function from the jsonlite library to load JSON (JavaScript Object Notation) files
- First, make sure to install and load the jsonlite library with install.packages("jsonlite") and library(jsonlite)
- Basic syntax: dataName <- jsonlite::fromJSON("path/to/your/file.json")
- Replace dataName with a meaningful variable name
- Replace the file path with the location of your specific JSON file
- Consider printing the first few rows of your data using head() to verify it loaded correctly to access specific parts of the JSON data
- Convert to a tibble (data frame) using as_tibble() if needed for better visual organisation

NOTE: Some JSON files have nested structures, so they might be more difficult to load.

```{r}

tumulusJSON <- jsonlite::fromJSON("../data/2025-03-10-EDCS_via_Lat_Epig-term1_tumulus-74.json")
tumulus <- as_tibble(tumulusJSON$data)

head(tumulus) # print the first six rows
```


## Load data from a EpiDoc file - single file

Used Claude.ai v3.7 to generate the code.

- This is to demonstrate hoe complex it is to extract any data out of XML, however useful format it is for epigraphy
- You need to know the structure of the XML and its tags to be able to extract any meaningful data
```{r}
# Install required packages if not already installed
if (!require("xml2")) install.packages("xml2")
if (!require("dplyr")) install.packages("dplyr")

# Load necessary libraries
library(xml2)
library(dplyr)

# Function to parse EpiDoc file to dataframe
parse_epidoc <- function(file_path) {
  # Read the XML file
  xml_data <- read_xml(file_path)
  
  # Get the namespace - this is critical for TEI XML
  ns <- xml_ns(xml_data)
  
  # Create an empty list to store our extracted data
  data_list <- list()
  
  # Extract basic info with namespace handling
  data_list$filename <- xml_text(xml_find_first(xml_data, "//d1:publicationStmt/d1:idno[@type='filename']", ns))
  data_list$title <- xml_text(xml_find_first(xml_data, "//d1:titleStmt/d1:title", ns))
  
  # Extract identifiers
  id_types <- c("filename", "TM", "EDR", "EDH", "EDCS", "PHI", "URI", "DOI")
  for (id_type in id_types) {
    xpath <- sprintf("//d1:publicationStmt/d1:idno[@type='%s']", id_type)
    node <- xml_find_first(xml_data, xpath, ns)
    if (!is.na(node)) {
      data_list[[paste0("id_", id_type)]] <- xml_text(node)
    }
  }
  
  # Extract editors
  editors <- xml_find_all(xml_data, "//d1:titleStmt/d1:editor", ns)
  if (length(editors) > 0) {
    data_list$editors <- paste(sapply(editors, xml_text), collapse = "; ")
  }
  
  # Extract location info
  origPlace <- xml_find_first(xml_data, "//d1:origin/d1:origPlace", ns)
  if (!is.na(origPlace)) {
    ancient_place <- xml_find_first(origPlace, "./d1:placeName[@type='ancient']", ns)
    if (!is.na(ancient_place)) {
      data_list$ancient_place <- xml_text(ancient_place)
      data_list$ancient_place_ref <- xml_attr(ancient_place, "ref")
      data_list$ancient_place_certainty <- xml_attr(ancient_place, "cert")
    }
    
    modern_place <- xml_find_first(origPlace, "./d1:placeName[@type='modern']", ns)
    if (!is.na(modern_place)) {
      data_list$modern_place <- xml_text(modern_place)
      data_list$modern_place_ref <- xml_attr(modern_place, "ref")
    }
    
    geo <- xml_find_first(origPlace, "./d1:geo", ns)
    if (!is.na(geo)) {
      coords <- xml_text(geo)
      data_list$geo_coords <- coords
      
      # Parse coordinates if they're in the "lat, long" format
      coords_split <- strsplit(coords, ",")[[1]]
      if (length(coords_split) == 2) {
        data_list$latitude <- as.numeric(trimws(coords_split[1]))
        data_list$longitude <- as.numeric(trimws(coords_split[2]))
      }
    }
  }
  
  # Extract dating info
  date_node <- xml_find_first(xml_data, "//d1:origin/d1:origDate", ns)
  if (!is.na(date_node)) {
    data_list$date_text <- xml_text(date_node)
    data_list$date_method <- xml_attr(date_node, "datingMethod")
    data_list$date_not_before <- xml_attr(date_node, "notBefore-custom")
    data_list$date_not_after <- xml_attr(date_node, "notAfter-custom")
  }
  
  # Extract material info
  material_node <- xml_find_first(xml_data, "//d1:support/d1:material", ns)
  if (!is.na(material_node)) {
    data_list$material <- xml_text(material_node)
    data_list$material_ref <- xml_attr(material_node, "ref")
  }
  
  # Extract language info
  lang_node <- xml_find_first(xml_data, "//d1:msContents/d1:textLang", ns)
  if (!is.na(lang_node)) {
    data_list$language <- xml_text(lang_node)
    data_list$main_lang <- xml_attr(lang_node, "mainLang")
  }
  
  # Extract repository/museum info
  repository_node <- xml_find_first(xml_data, "//d1:repository", ns)
  if (!is.na(repository_node)) {
    data_list$repository <- xml_text(repository_node)
    data_list$repository_role <- xml_attr(repository_node, "role")
    data_list$repository_ref <- xml_attr(repository_node, "ref")
  }
  
  # Extract inventory number
  inventory_node <- xml_find_first(xml_data, "//d1:idno[@type='inventory']", ns)
  if (!is.na(inventory_node)) {
    data_list$inventory <- xml_text(inventory_node)
  }
  
  # Extract inscription text
  ab_node <- xml_find_first(xml_data, "//d1:div[@type='edition']/d1:ab", ns)
  if (!is.na(ab_node)) {
    data_list$inscription_text <- xml_text(ab_node)
    # Get the language of the inscription text
    data_list$inscription_lang <- xml_attr(xml_find_first(xml_data, "//d1:div[@type='edition']", ns), "xml:lang")
  }
  
  # Extract named entities
  names <- xml_find_all(xml_data, "//d1:div[@type='edition']//d1:name", ns)
  if (length(names) > 0) {
    data_list$named_entities <- paste(sapply(names, xml_text), collapse = "; ")
  }
  
  # Extract line breaks information
  lb_nodes <- xml_find_all(xml_data, "//d1:div[@type='edition']//d1:lb", ns)
  if (length(lb_nodes) > 0) {
    line_numbers <- xml_attr(lb_nodes, "n")
    data_list$line_count <- length(lb_nodes)
    data_list$line_numbers <- paste(line_numbers, collapse = "; ")
  }
  
  # Extract abbreviations and their expansions
  abbr_nodes <- xml_find_all(xml_data, "//d1:div[@type='edition']//d1:abbr", ns)
  if (length(abbr_nodes) > 0) {
    abbreviations <- sapply(abbr_nodes, xml_text)
    data_list$abbreviations <- paste(abbreviations, collapse = "; ")
  }
  
  ex_nodes <- xml_find_all(xml_data, "//d1:div[@type='edition']//d1:ex", ns)
  if (length(ex_nodes) > 0) {
    expansions <- sapply(ex_nodes, xml_text)
    data_list$expansions <- paste(expansions, collapse = "; ")
  }
  
  # Extract gaps and unclear text
  gap_nodes <- xml_find_all(xml_data, "//d1:div[@type='edition']//d1:gap", ns)
  if (length(gap_nodes) > 0) {
    gap_info <- c()
    for (node in gap_nodes) {
      reason <- xml_attr(node, "reason")
      extent <- xml_attr(node, "extent")
      unit <- xml_attr(node, "unit")
      gap_info <- c(gap_info, paste(reason, extent, unit, sep=":"))
    }
    data_list$gaps <- paste(gap_info, collapse = "; ")
  }
  
  unclear_nodes <- xml_find_all(xml_data, "//d1:div[@type='edition']//d1:unclear", ns)
  if (length(unclear_nodes) > 0) {
    unclear_text <- sapply(unclear_nodes, xml_text)
    data_list$unclear_text <- paste(unclear_text, collapse = "; ")
  }
  
  # Extract bibliography references
  bibl_nodes <- xml_find_all(xml_data, "//d1:div[@type='bibliography']//d1:bibl", ns)
  if (length(bibl_nodes) > 0) {
    bibl_info <- c()
    for (node in bibl_nodes) {
      bibl_type <- xml_attr(node, "type")
      bibl_n <- xml_attr(node, "n")
      cited_range <- xml_text(xml_find_first(node, ".//d1:citedRange", ns))
      bibl_info <- c(bibl_info, paste(c(bibl_type, bibl_n, cited_range), collapse=":"))
    }
    data_list$bibliography <- paste(bibl_info, collapse = "; ")
  }
  
  # Convert list to dataframe, handling NULLs
  df <- as.data.frame(lapply(data_list, function(x) if(is.null(x)) NA else x), stringsAsFactors = FALSE)
  return(as_tibble(df))
}

# Process the file
file_path <- "../data/Epidoc/ISic0001.xml"  # Adjust this to your file path
inscription_df <- parse_epidoc(file_path)

# Print results
print(inscription_df)

# To show all columns in a readable format:
for (col in names(inscription_df)) {
  cat(paste0(col, ": ", inscription_df[[col]], "\n"))
}

# Save to CSV if needed
 write.csv(inscription_df, "../data/Epidoc_inscription_data.csv", row.names = FALSE)

```
`Exercise: Compare the original Epidoc file with the extracted information. What imporantant information (for you) is currently not being extracted? Can you try?`



## Load data from a EpiDoc file - multiple files

Used Claude.ai v3.7 to generate the code.

This section demonstrates how to extract information from multiple XML files. Be ware, that not every XML has the same structure, or contains the same attributes. You may need to modify the code, depending on your project and the structure of your data. Feel free to add AI as your coding partner (I personally recommend Claude.ai for coding).

```{r}
# Install required packages if not already installed
if (!require("xml2")) install.packages("xml2")
if (!require("dplyr")) install.packages("dplyr")

# Load necessary libraries
library(xml2)
library(dplyr)

# Enhanced parse_epidoc function with better handling of structural differences
parse_epidoc <- function(file_path) {
  # Read the XML file
  xml_data <- read_xml(file_path)
  
  # Get the namespace - this is critical for TEI XML
  ns <- xml_ns(xml_data)
  
  # Create an empty list to store our extracted data
  data_list <- list()
  
  # Always store the filename regardless of structure
  data_list$filename <- basename(file_path)
  
  # Function to safely extract text from XML nodes
  safe_xml_text <- function(xpath, default_value = NA) {
    node <- xml_find_first(xml_data, xpath, ns)
    if (!is.na(node)) {
      return(xml_text(node))
    } else {
      return(default_value)
    }
  }
  
  # Function to safely extract attribute from XML nodes
  safe_xml_attr <- function(xpath, attr_name, default_value = NA) {
    node <- xml_find_first(xml_data, xpath, ns)
    if (!is.na(node)) {
      attr_value <- xml_attr(node, attr_name)
      if (!is.na(attr_value)) {
        return(attr_value)
      }
    }
    return(default_value)
  }
  
  # Try multiple possible XPaths for common elements
  # For example, title might be in different locations
  title_xpaths <- c(
    "//d1:titleStmt/d1:title",
    "//tei:titleStmt/tei:title", 
    "//TEI:titleStmt/TEI:title",
    "//titleStmt/title"
  )
  
  for (xpath in title_xpaths) {
    tryCatch({
      value <- safe_xml_text(xpath)
      if (!is.na(value)) {
        data_list$title <- value
        break
      }
    }, error = function(e) {
      # Ignore errors and try next xpath
    })
  }
  
  # Extract identifiers - try different possible locations and attributes
  id_types <- c("filename", "TM", "EDR", "EDH", "EDCS", "PHI", "URI", "DOI")
  id_xpaths <- c(
    "//d1:publicationStmt/d1:idno[@type='%s']",
    "//tei:publicationStmt/tei:idno[@type='%s']",
    "//publicationStmt/idno[@type='%s']"
  )
  
  for (id_type in id_types) {
    for (xpath_template in id_xpaths) {
      xpath <- sprintf(xpath_template, id_type)
      tryCatch({
        value <- safe_xml_text(xpath)
        if (!is.na(value)) {
          data_list[[paste0("id_", id_type)]] <- value
          break
        }
      }, error = function(e) {
        # Ignore errors and try next xpath
      })
    }
  }
  
  # Similar approach for other elements...
  # [Add similar code blocks for other data you're extracting]
  
  # Attempt to extract inscription text from various possible locations
  inscription_xpaths <- c(
    "//d1:div[@type='edition']/d1:ab",
    "//tei:div[@type='edition']/tei:ab",
    "//div[@type='edition']/ab",
    "//d1:body//d1:div[@type='edition']",
    "//tei:body//tei:div[@type='edition']"
  )
  
  for (xpath in inscription_xpaths) {
    tryCatch({
      value <- safe_xml_text(xpath)
      if (!is.na(value)) {
        data_list$inscription_text <- value
        break
      }
    }, error = function(e) {
      # Ignore errors and try next xpath
    })
  }
  
  # Convert list to dataframe, handling NULLs
  df <- as.data.frame(lapply(data_list, function(x) if(is.null(x)) NA else x), stringsAsFactors = FALSE)
  return(as_tibble(df))
}

# Modified process function to handle structural variations
process_multiple_epidoc_files <- function(directory_path, pattern = "*.xml") {
  # Get list of XML files in the directory
  file_paths <- list.files(path = directory_path, pattern = pattern, full.names = TRUE)
  
  # Check if any files were found
  if (length(file_paths) == 0) {
    stop("No XML files found in the specified directory.")
  }
  
  # Initialize an empty list to store data frames
  all_inscriptions <- list()
  success_count <- 0
  failure_count <- 0
  
  # Process each file
  for (i in seq_along(file_paths)) {
    file_path <- file_paths[i]
    file_name <- basename(file_path)
    cat(sprintf("Processing file %d of %d: %s\n", i, length(file_paths), file_name))
    
    # Try to parse the file and handle errors gracefully
    result <- tryCatch({
      df <- parse_epidoc(file_path)
      all_inscriptions[[length(all_inscriptions) + 1]] <- df
      success_count <- success_count + 1
      "success"
    }, error = function(e) {
      cat(sprintf("  - Error processing %s: %s\n", file_name, e$message))
      failure_count <- failure_count + 1
      return("failure")
    })
  }
  
  # Combine all data frames into one
  cat(sprintf("\nProcessing complete: %d succeeded, %d failed\n", success_count, failure_count))
  
  if (length(all_inscriptions) > 0) {
    # Use bind_rows with .id parameter to track which file each row came from
    combined_df <- bind_rows(all_inscriptions)
    
    # Count unique values for each column to understand structural variations
    cat("\nColumn summary (number of unique values):\n")
    for (col in names(combined_df)) {
      if (!all(is.na(combined_df[[col]]))) {
        unique_count <- length(unique(combined_df[[col]]))
        missing_count <- sum(is.na(combined_df[[col]]))
        cat(sprintf("  %s: %d unique values, %d missing values (%.1f%%)\n", 
                    col, unique_count, missing_count, 
                    missing_count/nrow(combined_df)*100))
      }
    }
    
    return(combined_df)
  } else {
    stop("No files were successfully processed.")
  }
}

# Function to process multiple files
process_multiple_epidoc_files <- function(directory_path, pattern = "*.xml") {
  # Get list of XML files in the directory
  file_paths <- list.files(path = directory_path, pattern = pattern, full.names = TRUE)
  
  # Check if any files were found
  if (length(file_paths) == 0) {
    stop("No XML files found in the specified directory.")
  }
  
  # Initialize an empty list to store data frames
  all_inscriptions <- list()
  
  # Process each file
  for (i in seq_along(file_paths)) {
    file_path <- file_paths[i]
    cat(sprintf("Processing file %d of %d: %s\n", i, length(file_paths), basename(file_path)))
    
    # Try to parse the file and handle errors gracefully
    tryCatch({
      df <- parse_epidoc(file_path)
      all_inscriptions[[length(all_inscriptions) + 1]] <- df
    }, error = function(e) {
      cat(sprintf("Error processing file %s: %s\n", basename(file_path), e$message))
    })
  }
  
  # Combine all data frames into one
  if (length(all_inscriptions) > 0) {
    # Use bind_rows to handle different column sets
    combined_df <- bind_rows(all_inscriptions)
    return(combined_df)
  } else {
    stop("No files were successfully processed.")
  }
}

# Use the function - specify your directory containing XML files
directory_path <- "../data/Epidoc/"  # Adjust this to your directory path
file.exists(directory_path) 
list.files(directory_path, pattern = "*.xml")
inscriptions_df <- process_multiple_epidoc_files(directory_path)

# Print summary
cat(sprintf("Successfully processed %d inscription files.\n", nrow(inscriptions_df)))

# Save to CSV
write.csv(inscriptions_df, "../data/All_Epidoc_inscriptions.csv", row.names = FALSE)
```

You can load back in the CSV and explore the results of extraction process.

```{r}
ALL_epidoc<- read.csv("../data/All_Epidoc_inscriptions.csv")
ALL_epidoc
```

## Load parquet

Parquet is a different format of data.

*Latin Inscriptions in Space and Time* (LIST) https://zenodo.org/records/10473706

*Greek Inscriptions in Space and Time* (GIST)https://zenodo.org/records/10139110

```{r}
# LIST <- st_read_parquet("../path/to/dataset.parquet") # you need to download the dataset first
```


## HELP ME!

**Tips**

- Double-check file paths
- Ensure file names and extensions are correct
- Use meaningful variable names that describe your data
- Consider printing the first few rows of your data using head() to verify it loaded correctly

**Common Errors to Watch Out For**

- Incorrect file path
- Mismatched file type (CSV vs TSV, JSON, XML)
- Missing libraries
- Typos in file names or variable names


# Simple descriptive statistics: TUMULUS example

We will use previously loaded examples of inscritpions containing the Latin term  `tumulus`, as extracted from EDCS on 10 March 2025, using the LatEpig tool. https://github.com/mqAncientHistory/Lat-Epig

`Ballsun-Stanton B., Heřmánková P., Laurence R. LatEpig (version 2.0). GitHub. URL: https://github.com/mqAncientHistory/Lat-Epig/ DOI: 10.5281/zenodo.12036539`

This guide walks you through analyzing a dataset of inscriptions that mention the term 'tumulus' using R and the tidyverse package.

Use the variable name to see the full dataset:

```{r}
tumulus
```

## How many inscriptions

Use nrow() to count how many rows (inscriptions) are in your dataset:

```{r}
nrow(tumulus)
```

## What are the names of attributes (columns)

Use names() to see all the attributes (columns) in your dataset:

```{r}
names(tumulus)
```


## How many inscriptions on stone

Find out what materials are used: 
```{r}
# what material is used
tumulus %>% 
  count(material) 

```

Select stone inscriptions only - beware, the terminology is in Latin. 

```{r}
# filter only stone inscriptions
tumulus %>% 
  filter(material == "lapis")
```

```{r}
# create a new variable

stone<- tumulus %>% 
  filter(material == "lapis")
```

```{r}
# count number of inscriptions on stone
nrow(stone)
```

## How many funerary inscriptions

EDCS contains category an attribute called `status`, that contains a list of different conceptual categorisations mixed together. We will be looking only for one - funerary inscriptions.

```{r}
# count all the different values within status

tumulus %>% 
  count(status)

```

```{r}
# use regular expressions within R to search only for tituli sepulcrales - funerary inscriptions
tumulus %>% 
  filter(status %in% str_subset(status, "tituli sepulcrales")) # regular expressions
```


## Province distribution
```{r}
# count inscriptions by province

tumulus %>% 
  count(province)
```

## Unique findspots

```{r}
# count and sort findspots

tumulus %>% 
  count(place, sort = TRUE)
```

## Map
Now, we will be plotting the resulst on a map. First we need to prepare the geographical data

```{r}
# Convert latitude and longitude to numbers and eliminate missing values in one step
tumulus <- tumulus %>%
  mutate(
    latitude = as.numeric(latitude),
    longitude = as.numeric(longitude)
  ) %>%
  filter(!is.na(latitude) & !is.na(longitude))
```

Create Interactive Map, using leaflet package:

```{r}
# make a map

# Create interactive map with popup information
tumulus_map <- leaflet(tumulus, width="100%") %>%
  # Add different map tile options
  addProviderTiles("Esri.WorldImagery", group = "Satellite View") %>%
  addProviderTiles("OpenStreetMap.Mapnik", group = "Street Map") %>%
  
  # Set initial view
  setView(lng = 15.9239625, lat = 31.9515694, zoom = 4) %>%
  
  # Set map boundaries
  setMaxBounds(
    lat1 = 43.633977, lng1 = -11.227926, 
    lat2 = 35.133882, lng2 = 50.882336
  ) %>%
  
  # Add circles with popups
  addCircles(
    lng = ~longitude, 
    lat = ~latitude, 
    opacity = 0.9, 
    radius = 5, 
    fill = TRUE, 
    color = "red", 
    fillColor = "red",
    # Add popup with detailed information
    popup = ~paste(
      "<strong>Province:</strong>", province, "<br>",
      "<strong>Place:</strong>", place, "<br>",
      "<strong>Material:</strong>", material, "<br>",
      "<strong>Status:</strong>", status
    )
  ) %>% 
  
  # Add layer control to switch between map views
  addLayersControl(
    baseGroups = c("Satellite View", "Street Map"),
    options = layersControlOptions(collapsed = FALSE)
  ) %>%
  
  # Add detailed legend with source and metadata
  addLegend(
    position = "bottomright",
    colors = c("Red"),
    labels = c("Inscription (EDCS)"), 
    opacity = 1,
    title = paste(
       "<div style='font-size: 14px; font-weight: bold;'>",
    "Inscriptions Mentioning 'Tumulus'",
    "</div>",
    "<div style='font-size: 8px; line-height: 1.3;'>",
    "Source: LatEpig Tool (v2.0)<br>",
    "Authors: Ballsun-Stanton B., Heřmánková P., Laurence R.<br>",
    "Date of Extraction: 10 March 2025<br>",
    "DOI: 10.5281/zenodo.12036539",
    "</div>"
    )
  ) %>% 
  
  # Add scale bar
  addScaleBar(position = "bottomleft") 

# Display the map
tumulus_map

# Optional: Save the map as an HTML file for sharing
saveWidget(tumulus_map, file = "../figures/tumulus_inscriptions_map.html")


```

# Home assignment

This section is to help with your home assignment and with your project. 

## Typological composition

The solution works for EDCS dataset only - as you neeed to separate types of inscriptions from other categories in the status attribute first.

### Preprocessing of the LatEpig generated dataset

### Type of inscription 

First we need to split the categorisation of the `status` attribute:

Division the status keywords into their own attributes, base on their purpose:
1. inscription type
2. notation of a societal status of a person on the inscription
3. details about process of inscribing, execution

```{r}
inscription_type_list <- c("tituli sepulcrales", "tituli fabricationis", "inscriptiones christianae", "tituli sacri", "tituli possessionis", "tituli operum", "miliaria", "tituli honorarii", "signacula", "diplomata militaria", "leges", "defixiones", "termini", "reges", "signacula medicorum", "senatus consulta")

inscribing_process_list <- c("sigilla impressa", "litterae erasae", "litterae in litura", "tesserae nummulariae")

status_notation_list<- c("viri", "tria nomina", "mulieres", "nomen singulare", "liberti/libertae", "milites", "Augusti/Augustae", "ordo senatorius", "servi/servae", "officium/professio", "ordo decurionum", "sacerdotes pagani", "praenomen et nomen", "ordo equester", "seviri Augustales", "sacerdotes christiani")

language_form_list<- c("carmina")
```


```{r}
# processing the status labels
tumulus <- tumulus %>% 
  mutate(inscr_type = str_extract_all(pattern = paste(inscription_type_list, collapse="|"), string = tumulus$status)) %>% 
  mutate(status_notation = str_extract_all(pattern = paste(status_notation_list, collapse="|"), string = tumulus$status)) %>% 
  mutate(inscr_process = str_extract_all(pattern = paste(inscribing_process_list, collapse="|"), string = tumulus$status)) %>% 
  mutate(inscr_lang = str_extract_all(pattern = paste(language_form_list, collapse="|"), string = tumulus$status))
```

### Convert character (0) to NA

```{r}
tumulus$inscr_type <- lapply(tumulus$inscr_type, function(x) if(identical(x, character(0))) NA_character_ else x)
tumulus$status_notation <- lapply(tumulus$status_notation, function(x) if(identical(x, character(0))) NA_character_ else x)
tumulus$inscr_process <- lapply(tumulus$inscr_process, function(x) if(identical(x, character(0))) NA_character_ else x)
tumulus$inscr_lang <- lapply(tumulus$inscr_lang, function(x) if(identical(x, character(0))) NA_character_ else x)
```

### Overview of types of inscriptions

```{r}
tumulus_insctype<- unnest_longer(tumulus, col = "inscr_type")
```

```{r}
# number of instances and their ratio by inscription types

tumulus_insctype %>% 
  count(inscr_type, sort=T) %>% 
  mutate(ratio = n/(sum(n)/100))
```


## Temporal composition - rough overview

```{r}
dated <- tumulus %>% 
  dplyr::select(`EDCS-ID`, date_not_before, date_not_after) 
dated
```

```{r}
# start date is not empty
dated %>% 
  filter(date_not_before != "")
```


```{r}
# plotting the dating interval

dated %>% ggplot(aes(x = as.numeric(date_not_before), y = `EDCS-ID`, colour = date_not_before)) + geom_segment(aes(xend = as.numeric(dated$date_not_after), yend = dated$`EDCS-ID`), colour = "black") +
  geom_point(size = 1) +
  geom_point(aes(x = as.numeric(dated$date_not_after)), size = 1) +
  theme_bw() +
  theme(legend.position = "none") +
  labs(x = "Timeline", y = "Inscription EDCS ID", title = "Dating intervals of Latin inscriptions containing the term tumulus") +
  theme(axis.text=element_text(size=6),
        axis.title=element_text(size=12)) -> dated_intervals

dated_intervals
```

```{r}
# save as a figure
ggsave(plot = dated_intervals, width = 12, height = 10, dpi = 300, filename = "../figures/tumulus_dating.png")
```

